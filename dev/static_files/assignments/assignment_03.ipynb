{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "-woU4Sodh6ND",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Assignment #3\n",
    "\n",
    "\n",
    "Deep Learning / Fall 1398, Iran University of Science and Technology\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "JWitIy1viFuD",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Please pay attention to these notes:**\n",
    "\n",
    "<br/>\n",
    "- **Assignment Due: ** xxxx/xx/xx 23:59\n",
    "- If you need any additional information, please review the assignment page on the course website.\n",
    "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
    "```\n",
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################\n",
    "```\n",
    "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by himself/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences. So, please mention his/her name if you have a team-mate.\n",
    "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
    "- Finding any sort of copying will zero down that assignment grade and also will be counted as two negative assignment for your final score.\n",
    "- When you are ready to submit, please follow the instructions at the end of this notebook.\n",
    "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Forum page.\n",
    "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of its dependencies.\n",
    "- **Before starting to work on the assignment Please fill your name in the next section *AND Remember to RUN the cell.* **\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "Assignment Page: [https://iust-deep-learning.github.io/981/assignments/03_transfer_learning_and_sequence_to_sequence_models](https://iust-deep-learning.github.io/981/assignments/03_transfer_learning_and_sequence_to_sequence_models)\n",
    "\n",
    "Course Forum: [https://groups.google.com/forum/#!forum/dl981/](https://groups.google.com/forum/#!forum/dl981/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "AbbBbIm2LF6a",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "Vjwf6dWNCIQP",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Fill your information here & run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "NeLkOPE6Qwr7",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
    "student_id = 0 #@param {type:\"integer\"}\n",
    "student_name = \"\" #@param {type:\"string\"}\n",
    "Your_Github_account_Email = \"\" #@param {type:\"string\"}\n",
    "\n",
    "print(\"your student id:\", student_id)\n",
    "print(\"your name:\", student_name)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ASSIGNMENT_PATH = Path('asg03')\n",
    "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bRw9nz5aDEu"
   },
   "source": [
    "# Transfer learning warm up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jOV0dJvYayPT"
   },
   "source": [
    "Pre-trained networks are networks that are already trained and stored using a lot of data. We use these networks because:\n",
    "- Our data is usually not very large.\n",
    "- We do not need to start the training process from scratch.\n",
    "\n",
    "\n",
    "Usually, first layers in deep networks extract general features, and as we move forward in the layers, the network learns patterns more specific to the task. So if we freeze the first layers and update the weights of the last layers according to your data, the network can better learn the patterns in our data for a particular task in less time and using relatively low data.\n",
    "\n",
    "We now want to separate ambulance classes and ordinary cars using the VGG network. To do this, we use the data you generated for the previous assignment. You must generate those data in this assignment again and split the dataset into train and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Tb6FTQ2oPl3"
   },
   "outputs": [],
   "source": [
    "  ########################################\n",
    "  #             get dataset              #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2FY-dVhGp46S"
   },
   "outputs": [],
   "source": [
    "images = {'train': [], 'test': []} # Put the images here\n",
    "labels = {'train': [], 'test': []} # Put the labels here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_V3tN4RpUCJ"
   },
   "source": [
    "\t\n",
    "Now at first, we need to declare a VGG modal to extract the image features. You can see the structure and layers of the network below.\n",
    "A pre-trained model can extract the features of an image itself. So at first, please do this classification task just with Features extracted from the VGG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hov0mNGIKhu-"
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False)\n",
    "vgg16_model.summary()\n",
    "\n",
    "xs = {'train': [], 'test': []}\n",
    "for image in images['train']:\n",
    "  img = # first read image\n",
    "  img = cv2.resize(img, (224, 224))\n",
    "  img = np.expand_dims(img, axis=0)\n",
    "  img = preprocess_image(img)\n",
    "  features = vgg16_model.predict(img)\n",
    "  ff = features.flatten()\n",
    "  xs['train'].append(features)\n",
    "\n",
    "for image in images['test']:\n",
    "  img = # first read image\n",
    "  img = cv2.resize(img, (224, 224))\n",
    "  img = np.expand_dims(img, axis=0)\n",
    "  img = preprocess_image(img)\n",
    "  features = vgg16_model.predict(img)\n",
    "  ff = features.flatten()\n",
    "  xs['test'].append(features)\n",
    "\n",
    "ys = {'train': [], 'test': []}\n",
    "ys['train'] = labels['train'][:]\n",
    "ys['test'] = labels['test'][:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LsZzpujlq6_7"
   },
   "source": [
    "Finally, implement a simple Keras model to the classification of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UU3XD65Dqz2J"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjdu0lIVMDf9"
   },
   "source": [
    "Next, you need to do this task with fine-tuned model. How many layers of model do you think you need to update according to your data?\n",
    "\n",
    "Now implement fine-tuning on the model and repeat classifying data with the same model as the previous part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p42a4sSLU3cx"
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False)\n",
    "vgg16_model.summary()\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDHu4pnDQKDP"
   },
   "source": [
    "# 1. Multi objective Sequence to Sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IHiICc7PqKE"
   },
   "source": [
    "Remember the **Transfer Learning and Sequence to Sequence Model** practical class; you learned how to use a pre-trained model, and how to train a sequence to sequence model. In this task, you are going to build a sequence to sequence model by feeding English characters into your model and predicting French and Persian characters. There are some criteria that you must consider:\n",
    "1. You have learned about building a character level sequence to sequence model in the [practical class](https://iust-deep-learning.github.io/981/practical/). You can review the full instruction and the code [here](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) (We suggest practicing with [this](https://colab.research.google.com/drive/1LS0jeq6glTKoY7zRyQBvbBHmH-jKowZO) notebook before jumping to the answer). For your homework, you must implement a character-level multi-objective sequence to sequence model, which translates English texts to French and Persian Simultaneously. You can learn about building neural networks with multiple outputs [here](https://sanjayasubedi.com.np/deeplearning/multioutput-keras/).  \n",
    "\n",
    "2. After training the model, you **must** use the prediction and test some samples, as it was provided in your practical class notebook.\n",
    "\n",
    "3. You **must** use the dataset which was used in your practical class. For Persian translation, you must translate it from the English part by using any translation APIs such as [Google Translate](https://cloud.google.com/translate/docs/).\n",
    "\n",
    "4. Remember saving all of your trained models under the path 'ASSIGNMENT_PATH /'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "QfSFVOaKvlDb",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 2. BERT and Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBsBGPfYXK_b"
   },
   "source": [
    "In computational linguistics, word-sense disambiguation (WSD) is an open problem concerned with identifying which sense of a word is used in a sentence. Imagine a bank of water, or a bank which accepts deposits. You can distinguish these two \"banks\" easily because you understand the context of the sentence which is used for it. What if we want to distinguish senses of a word automatically? You have learned about word embeddings and W2V from the [first assignment](https://iust-deep-learning.github.io/981/assignments/01_mlp_and_preprocessing). W2V extracts a unique vector that is a representation of that word. That is sufficiant we are going distinguish between two different words, such as \"Hi\" and \"Bye\"; but we cannot purely depend on it when we want to distinguish between a specific word with several senses.\n",
    "\n",
    "Researchers have developed several methods for tackling this issue. One of these techniques named DeConf is proposed by your professor, Dr. Pilehvar. He tackled this problem by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. You can read the complete paper [here](https://arxiv.org/abs/1608.01961). Besides, A full survey about WSD is available [here](https://www.researchgate.net/publication/220566219_Word_Sense_Disambiguation_A_Survey).\n",
    "\n",
    "In this task, we are going to distinguish between two senses of the word (noun) \"bat\" using [BERT](https://arxiv.org/abs/1810.04805). BERT is a context-aware language representation model that can produce different vectors for a specific word according to the context of the sentence.\n",
    "\n",
    "According to [Cambridge dictionary](https://dictionary.cambridge.org/dictionary/english/bat), \"bat\" has two meanings:\n",
    "1. A specially shaped piece of wood used for hitting the ball in some games (cricket bat)\n",
    "\n",
    "2. A small animal like a mouse with wings that flies at night (mammal bat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJnVfh33_gl0"
   },
   "source": [
    "We use two simple datasets for our task. First, cricketbat.txt, which contains few sentences referring to bat used in cricket sport, and second, vampirebat.txt, which contains few sentences referring to the mammal bird bat. These datasets are available in [this](https://github.com/omkar-dsd/mini_projects/tree/master/word_sense_disambiuation) repository, which is used for another WSD approach using [Wordnet](https://wordnet.princeton.edu/). The full explanation of WSD using this method is available [here](https://towardsdatascience.com/a-simple-word-sense-disambiguation-application-3ca645c56357)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6I_xqKnD_ctX"
   },
   "outputs": [],
   "source": [
    "# Download and load the datasets here. Then, separate and lower the sentences and insert them in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HbDyDZTGAO9r"
   },
   "source": [
    "Now, we use BERT to encode words of these datasets. You can use [this](https://github.com/hanxiao/bert-as-service) repository to extract word embeddings of the sentences. Use BERT-Base, Uncased as your pre-trained BERT model. Then, insert the vectors of all words with root \"bat\" in another list for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUKVhOpEDy3G"
   },
   "outputs": [],
   "source": [
    "# Use bert to extract the embeddings of \"bat\"s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEACvY9JD5m2"
   },
   "source": [
    "Now, you need to find a specific embedding for each sense of the \"bat\". A simple approach is averaging all \"bat\" vectors of each document. Then, you should calculate the distance between the BERT embedding of \"bat\" in our test cases with your document \"bat\"s. \n",
    "\n",
    "Hint: [This](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html) function may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "spLXPJgoEa-s"
   },
   "outputs": [],
   "source": [
    "test_cases = [\"bats are mammals\",\"bats breed\",\"bats are used to play cricket\", \"which bat has handle?\",\"bats can fly\"]\n",
    "test_labels = [\"mammal bat\",\"mammal bat\",\"cricket bat\",\"cricket bat\",\"mammal bat\"]\n",
    "\n",
    "your_labels = []\n",
    "\n",
    "#fill your labels with the \"bat\" with minimum distance to the test case\n",
    "\n",
    "assert test_labels==your_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YirXln2VHDXD"
   },
   "source": [
    "Hmm... It makes sense. Let's see what you produced. You should use [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) to compress the 768 dimensions of BERT embeddings into just 2 dimensions. Then, use matplotlib to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26-SEjdVIxhI"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "# make a list of embeddings like this: [ALL_CRICKET_BAT_EMBEDDINGS,ALL_MAMMAL_BAT_EMBEDDINGS,AVERAGE_CRICKET_BAT_EMBEDDING,AVERAGE_MAMMAL_BAT_EMBEDDING,ALL_TEST_BAT_EMBEDDINS]\n",
    "vec_all = []\n",
    "pca = TruncatedSVD(n_components=2)\n",
    "principalComponents = pca.fit_transform(vec_all)\n",
    "print(pca.explained_variance_ratio_)\n",
    "principalDf = pd.DataFrame(data=principalComponents\n",
    "                           , columns=['principal component 1', 'principal component 2'])\n",
    "\n",
    "\n",
    "\n",
    "# make a colors list. use \"b\" for CRICKET_BATs, \"r\" for MAMMAL_BATs\n",
    "colors =[]\n",
    "\n",
    "#extract pc1 and pc2 for document bats\n",
    "pc1_document=[]\n",
    "pc2_document=[]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('Principal Component 1', fontsize=15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize=15)\n",
    "for pc1, pc2, color in zip( pc1_document,pc2_document, colors):\n",
    "    ax.scatter(pc1, pc2, s=50, color=color)\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCi9RTNGLCrP"
   },
   "source": [
    "It seems that despite this considerable compression, the clusters are distinguishable. Now, for the last part, visualize the average vectors and the test case vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNMk9sXAL23l"
   },
   "outputs": [],
   "source": [
    "#extract pc1 and pc2 for average and test bats\n",
    "pc1_remainings=[]\n",
    "pc2_remainings=[]\n",
    "\n",
    "colors =[\"b\",\"r\",\"m\",\"m\",\"c\",\"c\",\"m\"]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('Principal Component 1', fontsize=15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize=15)\n",
    "for pc1, pc2, color in zip( pc1_remainings,pc2_remainings, colors):\n",
    "    ax.scatter(pc1, pc2, s=50, color=color)\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "pBjq-MvamPXO",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "7o4q5LiFiOx1",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instruction:\n",
    "\n",
    "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
    "2. Select File > Save.\n",
    "3. Run **Create Submission** cell, It may take several minutes and it may ask you for your credential.\n",
    "4. Run **Download Submission** cell to obtain your submission as a zip file.\n",
    "5. Grab downloaded file (`dl_asg03__xx__xx.zip`) and submit it via [https://forms.gle/W8AMoNffho8TQLB87](https://forms.gle/W8AMoNffho8TQLB87)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "iWRUf35av3ZP",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Note: ** We need your Github token to create (if doesn't exist previously) new repository to store learned model data. Also Google Drive token enables us to download the current notebook & create a submission. If you are interested feel free to check our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "cTytERc-vlaK",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Create Submission (Run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "r4o37hc3AEUg",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "! pip install -U --quiet PyDrive > /dev/null\n",
    "! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
    "  \n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "from google.colab import files\n",
    "from IPython.display import Javascript\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "asg_name = 'assignment_03'\n",
    "script_save = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "repo_name = 'iust-deep-learning-assignments'\n",
    "submission_file_name = 'dl_asg03__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
    "\n",
    "! tar xf hub-linux-amd64-2.10.0.tgz\n",
    "! cd hub-linux-amd64-2.10.0/ && chmod a+x install && ./install\n",
    "! hub config --global hub.protocol https\n",
    "! hub config --global user.email \"$Your_Github_account_Email\"\n",
    "! hub config --global user.name \"$student_name\"\n",
    "! hub api --flat -X GET /user\n",
    "! hub api -F affiliation=owner -X GET /user/repos > repos.json\n",
    "\n",
    "repos = json.load(open('repos.json'))\n",
    "repo_names = [r['name'] for r in repos]\n",
    "has_repository = repo_name in repo_names\n",
    "if not has_repository:\n",
    "  get_ipython().system_raw('! hub api -X POST -F name=%s /user/repos > repo_info.json' % repo_name)\n",
    "  repo_info = json.load(open('repo_info.json')) \n",
    "  repo_url = repo_info['clone_url']\n",
    "else:\n",
    "  for r in repos:\n",
    "    if r['name'] == repo_name:\n",
    "      repo_url = r['clone_url']\n",
    "  \n",
    "stream = open(\"/root/.config/hub\", \"r\")\n",
    "token = list(yaml.load_all(stream))[0]['github.com'][0]['oauth_token']\n",
    "repo_url_with_token = 'https://'+token+\"@\" +repo_url.split('https://')[1]\n",
    "\n",
    "! git clone \"$repo_url_with_token\"\n",
    "! cp -r \"$ASSIGNMENT_PATH\" \"$repo_name\"/\n",
    "! cd \"$repo_name\" && git add -A\n",
    "! cd \"$repo_name\" && git commit -m \"Add assignment 03 results\"\n",
    "! cd \"$repo_name\" && git push -u origin master\n",
    "\n",
    "sub_info = {\n",
    "    'student_id': student_id,\n",
    "    'student_name': student_name, \n",
    "    'repo_url': repo_url,\n",
    "    'asg_dir_contents': os.listdir(str(ASSIGNMENT_PATH)),\n",
    "    'dateime': str(time.time()),\n",
    "    'asg_name': asg_name\n",
    "}\n",
    "json.dump(sub_info, open('info.json', 'w'))\n",
    "\n",
    "Javascript(script_save)\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
    "\n",
    "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
    "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
    "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
    "\n",
    "print(\"##########################################\")\n",
    "print(\"Done! Submisson created, Please download using the bellow cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "mX9OFzaLtYu_",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Submission (Run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "PUzTlnX1nS8X",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "files.download(submission_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhPXb5goP5Yw"
   },
   "source": [
    "If that cell makes an error when running you can download file dl_asg03_your_struden_id_your_name.zip from left panel and files section by right-clicking on it and choosing download button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9KaPEsgPruT"
   },
   "source": [
    "# Special Thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOXZZrFjPxWO"
   },
   "source": [
    "Special thanks to Amirhossein Kazemnejad and Kiamehr Razaee for creating the template of deep learning course assignments."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
