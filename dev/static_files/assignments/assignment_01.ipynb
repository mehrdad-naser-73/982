{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "-woU4Sodh6ND",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Assignment #1\n",
    "\n",
    "\n",
    "Deep Learning / Fall 1398, Iran University of Science and Technology\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "JWitIy1viFuD",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Please pay attention to these notes:**\n",
    "\n",
    "<br/>\n",
    "- **Assignment Due: ** 1398/08/18 23:59\n",
    "- If you need any additional information, please review the assignment page on the course website.\n",
    "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
    "```\n",
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################\n",
    "```\n",
    "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by himself/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences. So, please mention his/her name if you have a team-mate.\n",
    "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
    "- Finding any sort of copying will zero down that assignment grade and also will be counted as two negative assignment for your final score.\n",
    "- When you are ready to submit, please follow the instructions at the end of this notebook.\n",
    "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Forum page.\n",
    "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of its dependencies.\n",
    "- **Before starting to work on the assignment Please fill your name in the next section *AND Remember to RUN the cell.* **\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "Assignment Page: [https://iust-deep-learning.github.io/981/assignments/01_mlp_and_preprocessing](https://iust-deep-learning.github.io/981/assignments/01_mlp_and_preprocessing)\n",
    "\n",
    "Course Forum: [https://groups.google.com/forum/#!forum/dl981/](https://groups.google.com/forum/#!forum/dl981/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "0ejALNiDCWnd",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "Vjwf6dWNCIQP",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Fill your information here & run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "NeLkOPE6Qwr7",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
    "student_id = 0 #@param {type:\"integer\"}\n",
    "student_name = \"\" #@param {type:\"string\"}\n",
    "Your_Github_account_Email = \"\" #@param {type:\"string\"}\n",
    "\n",
    "print(\"your student id:\", student_id)\n",
    "print(\"your name:\", student_name)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ASSIGNMENT_PATH = Path('asg01')\n",
    "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "QfSFVOaKvlDb",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 1. MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "UmprL4aETrj8",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In class, we studied about MLP. In this part, you have to implement your own MLP and train and test it on the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "NLU26ncrYZL5",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Iris dataset\n",
    "---\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.\n",
    "\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n",
    "\n",
    "You can see [this](https://en.wikipedia.org/wiki/Iris_flower_data_set) link for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "bAbR3KNYN9Yw",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's get this simple dataset and see some samples of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {},
    "colab_type": "code",
    "id": "iRVN68kaOFXX",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(iris['data'][:10])\n",
    "print(iris['target'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otcIUS2gUwls"
   },
   "source": [
    "### Implementation\n",
    "---\n",
    "Before going any further, we have to import some prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AgTnW5PUyCj"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dipt3Y3fva8"
   },
   "source": [
    "If you want to import some modules or implement some helper functions or classes you can do it in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2klj-T2sf7aZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "kr2IFQDgOlDD",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, implement your MLP from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHey_Vq5U5sj"
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "  \n",
    "  def train(self, x, y):\n",
    "    \"\"\"\n",
    "    train MLP model on train data\n",
    "\n",
    "    Args:\n",
    "      x: 2d numpy array or list of train data\n",
    "      y: 1d or 2d numpy array or list of train data labels\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    return True\n",
    "  \n",
    "  def test(self, x, y):\n",
    "    \"\"\"\n",
    "    test MLP model on test data\n",
    "\n",
    "    Args:\n",
    "      x: 2d numpy array or list of test data\n",
    "      y: 1d or 2d numpy array or list of test data labels\n",
    "\n",
    "    Returns:\n",
    "      acc: In the simplest way ratio between the number of correct predicts with the number \n",
    "           of all train data\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    return acc\n",
    "  \n",
    "  def predict(self, x):\n",
    "    \"\"\"\n",
    "    predict output of MLP model on input data\n",
    "\n",
    "    Args:\n",
    "      x: 1d or 2d numpy array or list of input data\n",
    "\n",
    "    Returns:\n",
    "      pred: 1d numpy array or list or integer that represent output predicted \n",
    "            from MLP\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    return pred\n",
    "  \n",
    "  def save_model(self, model_path):\n",
    "    \"\"\"\n",
    "    save model to disk\n",
    "\n",
    "    Args:\n",
    "      model_path: path of model\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    return True\n",
    "  \n",
    "  def load_model(self, model_path):\n",
    "    \"\"\"\n",
    "    load model from disk\n",
    "\n",
    "    Args:\n",
    "      model_path: path of model\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwi-sxIHWC5A"
   },
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "  \"\"\"\n",
    "  initilize a MLP model that classify Iris dataset\n",
    "  \n",
    "  Returns:\n",
    "    model: A MLP object\n",
    "               \n",
    "  Hint: Consider the number of features in the Iris dataset and the number of its classes \n",
    "        and initialize weights.\n",
    "  \"\"\"\n",
    "  \n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dOvmFMbQWTSW"
   },
   "outputs": [],
   "source": [
    "def split_train_test(x, y):\n",
    "  \"\"\"\n",
    "  split input data and labels to train and test sections.\n",
    "  \n",
    "  Args:\n",
    "    x: 2d numpy array or list of input data\n",
    "    y: 1d or 2d numpy array or list of data labels\n",
    "    \n",
    "  Returns:\n",
    "    train_data: 2d numpy array or list of train_data\n",
    "    train_labels: 1d or 2d numpy array or list of train data labels\n",
    "    test_data: 2d numpy array or list of test_data\n",
    "    test_labels: 1d or 2d numpy array or list of test data labels\n",
    "  \"\"\"\n",
    "  \n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################\n",
    "  \n",
    "  return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6AWxMe8Wr_X"
   },
   "source": [
    "Test your implementation(don't  change this cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMsQbydBW3FS"
   },
   "outputs": [],
   "source": [
    "mlp = initialize_model()\n",
    "train_data, train_labels, test_data, test_labels = split_train_test(iris['data'], iris['target'])\n",
    "mlp.train(train_data, train_labels)\n",
    "mlp.save_model(ASSIGNMENT_PATH / 'my_model.h5')\n",
    "del mlp\n",
    "new_mlp = initialize_model()\n",
    "new_mlp.load_model(ASSIGNMENT_PATH / 'my_model.h5')\n",
    "print('your model accuracy on test data is: %s' % (new_mlp.test(test_data, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "tFqWPWvot29x",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In class, we studied the mathematics behind the back-propagation when the activation function of the last layer is Relu. Now write equations related to the softmax activation function and obtain delta formulas for all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "Ob0fG9c9ws5w",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "$\\color{red}{\\text{Write your answer here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "toUNxrGEqh3p"
   },
   "source": [
    "# 2.Text classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xa4uMJhJhGRl"
   },
   "source": [
    "In class, we studied how to build a basic dense model. Now we want to learn how to prepare a text dataset to feed into a provided model. First, we start with a simple dataset and then, we try a harder example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGSdDSx7sm6z"
   },
   "source": [
    "## Sentiment Analysis on Movie Reviews \n",
    "This small dataset is available for free on NLTK. You can learn how to install `movie_reviews` dataset [here](https://www.nltk.org/data.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80jIfriaqKcz"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Input, Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6-ma5hOvv3Y"
   },
   "source": [
    "In every deep learning task, we need to divide our dataset into train and test categories. The train category is used to train the model, and the test one is used to evaluate the trained model. The proportion of train and test dataset does not have any specific formula, and it is up to you, but you should consider the majority of the dataset as the train one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoZMUKN4xZjq"
   },
   "outputs": [],
   "source": [
    "  '''\n",
    "    Split the documents into train and test datasets\n",
    "  '''\n",
    "document = {'train': [], 'test': []} # Put the documents here\n",
    "labels = {'train': [], 'test': []} # Put the labels here\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOSiSa1uzf61"
   },
   "source": [
    "### Encoding the text data \n",
    "\n",
    "To feed the text data into a deep model, we must convert the strings to numerical data. A variety of approaches are available for this purpose, and we use two of them for this task: **One-Hot** and **TF-IDF** encodings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzAxV-031voc"
   },
   "source": [
    "### One-Hot encoding\n",
    "\n",
    "one-hot is a group of bits among which the legal combinations of values are only those with a single high (1) bit and all the others low (0). So, in our case, we should convert each word to an array in which only one cell in the whole array must be 1, the one which represents that specific word. Then, to represent a document as a vector, we should sum all the words' vectors in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDhaJXZL4FcB"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "     Encode documents to One-Hot representation.\n",
    "'''\n",
    "xs = {'train': [], 'test': []} # Put the document vectors here\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dI7qsdXB5WFw"
   },
   "source": [
    "As we studied in the TA class, for classification tasks we need to convert the labels into the one-hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYphZx5g6ILu"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "     Convert labels into One-Hot representation.\n",
    "'''\n",
    "ys = {'train': [], 'test': []} # Put the label vectors here\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h6g6utE0ANO6"
   },
   "source": [
    "Now we build and train the model, and then visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEZSEKlrATyQ"
   },
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "\n",
    "    Source\n",
    "    ------\n",
    "    https://github.com/fchollet/keras/issues/5400#issuecomment-314747992\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    \"\"\"Calculate the F1 score.\"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r))\n",
    "\n",
    "def create_model(nb_classes, input_shape):\n",
    "    \"\"\"Create a MLP model.\"\"\"\n",
    "    input_ = Input(shape=input_shape)\n",
    "    x = input_\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dense(nb_classes)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    model = Model(inputs=input_, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YTw8Of5Aczx"
   },
   "outputs": [],
   "source": [
    "data = {'x_train': xs['train'], 'y_train': ys['train'],\n",
    "        'x_test': xs['test'], 'y_test': ys['test']}\n",
    "model = create_model(2, (data['x_train'].shape[1], ))\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\", metrics=[\"accuracy\",f1,recall,precision])\n",
    "history = model.fit(data['x_train'], data['y_train'],\n",
    "              batch_size=32,\n",
    "              epochs=20,\n",
    "              validation_data=(data['x_test'], data['y_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsqYhgagBS3v"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "acc = history_dict['acc']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "acc_values = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rR0h5HWPB5Zt"
   },
   "source": [
    "### TF-IDF encoding\n",
    "\n",
    "TFIDF, short for term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus[[1]](http://i.stanford.edu/~ullman/mmds/ch1.pdf). TF-IDF considers both frequencies of a word in the document and Inverse Document Frequency which determines whether a word is common in documents or not. You can learn more about this approach [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to implement it. Note that you need to provide a vector for each document with the same shape as the One-Hot vector but with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3gxirDVJErk2"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "     Encode documents to TF-IDF representation.\n",
    "'''\n",
    "xs = {'train': [], 'test': []} # Put the document vectors here\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szhsDbzVGd6i"
   },
   "source": [
    "Now we train and visualize our model again. Note that the result may vary concerning the preprocessing you do or the tokenizer you use to split your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXP8i9uZGyTo"
   },
   "outputs": [],
   "source": [
    "data = {'x_train': xs['train'], 'y_train': ys['train'],\n",
    "        'x_test': xs['test'], 'y_test': ys['test']}\n",
    "model = create_model(2, (data['x_train'].shape[1], ))\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\", metrics=[\"accuracy\",f1,recall,precision])\n",
    "history = model.fit(data['x_train'], data['y_train'],\n",
    "              batch_size=32,\n",
    "              epochs=20,\n",
    "              validation_data=(data['x_test'], data['y_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adscXEshG4zh"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['acc']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "acc_values = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xR6EHdIbNKMN"
   },
   "source": [
    "## Sentiment Analysis on IMDB\n",
    "\n",
    "Working with this dataset is a bit tricky. download the dataset from [here](https://ai.stanford.edu/~amaas/data/sentiment/), then use the Training set as your whole dataset. You can use a sample of 12500 reviews if you faced any ram problems, but remember to include both negative and positive reviews equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nVien8_kOfEQ"
   },
   "outputs": [],
   "source": [
    " '''\n",
    "    Import necessary modules, download and prepare the requested dataset\n",
    "  '''\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1igOnvcbO0GA"
   },
   "outputs": [],
   "source": [
    " '''\n",
    "    Split the documents into train and test datasets\n",
    "  '''\n",
    "document = {'train': [], 'test': []} # Put the documents here\n",
    "labels = {'train': [], 'test': []} # Put the labels here\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ANcY_UJP9EY"
   },
   "source": [
    "Now you train the dense model on this dataset. Use one of the encoding approaches you used for the prior dataset and then feed the preprocessed data into the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNcB0BmAQ50T"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "     Encode documents to a vector representation.\n",
    "'''\n",
    "xs = {'train': [], 'test': []} # Put the document vectors here\n",
    "ys = {'train': [], 'test': []} # Put the label vectors here\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3--WpasERTmI"
   },
   "outputs": [],
   "source": [
    "data = {'x_train': xs['train'], 'y_train': ys['train'],\n",
    "        'x_test': xs['test'], 'y_test': ys['test']}\n",
    "model = create_model(2, (data['x_train'].shape[1], ))\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\", metrics=[\"accuracy\",f1,recall,precision])\n",
    "history = model.fit(data['x_train'], data['y_train'],\n",
    "              batch_size=32,\n",
    "              epochs=20,\n",
    "              validation_data=(data['x_test'], data['y_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPyTfnrmRcHH"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['acc']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "acc_values = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLUsZDrkRhyo"
   },
   "source": [
    "### Word Embeddings\n",
    "In this section, we want to use a pre-trained word embedding to encode the reviews. To do so, we leverage the Google News Word2Vec model, a model that provides 300 semantic features for each word. These features are extracted concerning the position of the training word and by considering adjacent words in the training data (Google News). More detailed information will be discussed in your class later.\n",
    "\n",
    "You can download the pre-trained model from [here](https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz), and you may want to use [gensim](https://radimrehurek.com/gensim/) to load the file. Next, you need to replace the document vector with the average of word vectors that are available in the W2V model. Use a weighted average to consider the frequency of a word as well as its presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1YZjxO2uUd_0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "     Encode documents to a vector representation.\n",
    "'''\n",
    "xs = {'train': [], 'test': []} # Put the document vectors here\n",
    "ys = {'train': [], 'test': []} # Put the label vectors here\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QogSlZjVlb6"
   },
   "outputs": [],
   "source": [
    "data = {'x_train': xs['train'], 'y_train': ys['train'],\n",
    "        'x_test': xs['test'], 'y_test': ys['test']}\n",
    "model = create_model(2, (data['x_train'].shape[1], ))\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\", metrics=[\"accuracy\",f1,recall,precision])\n",
    "history = model.fit(data['x_train'], data['y_train'],\n",
    "              batch_size=32,\n",
    "              epochs=20,\n",
    "              validation_data=(data['x_test'], data['y_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtBMBnkrVn_D"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['acc']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "acc_values = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "zeDLckyY_Yuq",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 3. Image classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "Xl8ZYWYb6CD-",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this part, We want to classify animal images according to their species (frog vs penguin).\n",
    "\n",
    "First, we should download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFcAjcsYkmum"
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "! wget -q http://iust-deep-learning.github.io/981/static_files/assignments/asg01_assets/data.zip\n",
    "  \n",
    "# Then, Extact it\n",
    "! unzip data.zip -d .\n",
    "! cat frog_url.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p3GGwUnuk7nL"
   },
   "source": [
    "As you see, two files have the URL address of images, so you should download and save them in appropriate folders. Do it in this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "te1V5YkglNCG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cR4k4lq9ltef"
   },
   "source": [
    "As a suggestion, it is better to view some of the images at first. To do so, modify this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzKqluHImCT2"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_path = ''\n",
    "img = cv2.imread(img_path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lZIpkWWocwl"
   },
   "source": [
    "Before going any further, we have to import some prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ytC3UWEodnm"
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Bz5l-eJn9zG"
   },
   "source": [
    "In every deep learning task, we need to divide our dataset into train and test categories. The train category is used to train the model, and the test one is used to evaluate the trained model. The proportion of train and test dataset does not have any specific formula, and it is up to you, but you should consider the majority of the dataset as the train one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yetgwkdfn-Vd"
   },
   "outputs": [],
   "source": [
    "  '''\n",
    "    Split the images into train and test datasets\n",
    "  '''\n",
    "images = {'train': [], 'test': []} # Put the images here\n",
    "labels = {'train': [], 'test': []} # Put the labels here\n",
    "\n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_POcPVJ3p5SZ"
   },
   "source": [
    "Now we change images to numeric feature vectors to feed them into the network.\n",
    "\n",
    "To do so, we leverage the vgg16 model. It is a CNN model; these models will be discussed in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FdFUx4onq1ke"
   },
   "outputs": [],
   "source": [
    "vgg16_model = VGG16(weights='imagenet', include_top=False)\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ajha6oTkrIUD"
   },
   "source": [
    "To prepare images to feed them into the network, some preprocessing is required. Implement this in this cell. For example, you can normalize images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gv4_ZHMjrtjz"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    preprocess input image\n",
    "\n",
    "    Args:\n",
    "      image: 2d numpy array input image\n",
    "\n",
    "    Returns:\n",
    "      img: 2d numpy array preprocessed image\n",
    "    \"\"\"\n",
    "    img = image.copy()\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViiuhnAjr50a"
   },
   "source": [
    "Now, you must first preprocess the images, then convert/encode them into feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fv0ON92sr5Aj"
   },
   "outputs": [],
   "source": [
    "xs = {'train': [], 'test': []}\n",
    "for image in images['train']:\n",
    "    img = # first read image\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_image(img)\n",
    "    features = vgg16_model.predict(img)\n",
    "    ff = features.flatten()\n",
    "    xs['train'].append(features)\n",
    "\n",
    "for image in images['test']:\n",
    "    img = # first read image\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_image(img)\n",
    "    features = vgg16_model.predict(img)\n",
    "    ff = features.flatten()\n",
    "    xs['test'].append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8pRFojytQ0S"
   },
   "source": [
    "If you need to convert the labels into another format, you can do so by deleting the two last lines and implementing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjKjuiI3tcgB"
   },
   "outputs": [],
   "source": [
    "ys = {'train': [], 'test': []}\n",
    "ys['train'] = labels['train'][:]\n",
    "ys['test'] = labels['test'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-u2XPRdhuq5H"
   },
   "source": [
    "Now implement an MLP model for this task to separate frog images from penguin images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaEYxYW7vEyh"
   },
   "source": [
    "If you want to import some modules or implement some helper functions or classes you can do it in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rAxGzrWvEy8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "3mPZERqlvEzN",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, implement your MLP from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FS4YAJ-UvEzR"
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "  \n",
    "  def train(self, x, y):\n",
    "    \"\"\"\n",
    "    train MLP model on train data\n",
    "\n",
    "    Args:\n",
    "      x: 2d numpy array or list of train data\n",
    "      y: 1d or 2d numpy array or list of train data labels\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    return True\n",
    "  \n",
    "  def test(self, x, y):\n",
    "    \"\"\"\n",
    "    test MLP model on test data\n",
    "\n",
    "    Args:\n",
    "      x: 2d numpy array or list of test data\n",
    "      y: 1d or 2d numpy array or list of test data labels\n",
    "\n",
    "    Returns:\n",
    "      acc: In the simplest way ratio between the number of correct predicts with the number \n",
    "           of all train data\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    return acc\n",
    "  \n",
    "  def predict(self, x):\n",
    "    \"\"\"\n",
    "    predict output of MLP model on input data\n",
    "\n",
    "    Args:\n",
    "      x: 1d or 2d numpy array or list of input data\n",
    "\n",
    "    Returns:\n",
    "      pred: 1d numpy array or list or integer that represent output predicted \n",
    "            from MLP\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    return pred\n",
    "  \n",
    "  def save_model(self, model_path):\n",
    "    \"\"\"\n",
    "    save model to disk\n",
    "\n",
    "    Args:\n",
    "      model_path: path of model\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    return True\n",
    "  \n",
    "  def load_model(self, model_path):\n",
    "    \"\"\"\n",
    "    load model from disk\n",
    "\n",
    "    Args:\n",
    "      model_path: path of model\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################\n",
    "    #     Put your implementation here     #\n",
    "    ########################################\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMkLre5GvEze"
   },
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "  \"\"\"\n",
    "  initilize a MLP model that classify Iris dataset\n",
    "  \n",
    "  Returns:\n",
    "    model: A MLP object\n",
    "               \n",
    "  Hint: Consider the number of features in the Iris dataset and the number of its classes \n",
    "        and initialize weights.\n",
    "  \"\"\"\n",
    "  \n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trLVIeBcvEzm"
   },
   "source": [
    "Evaluate your model(don't change this cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VmzZgRSuvEzo"
   },
   "outputs": [],
   "source": [
    "mlp = initialize_model()\n",
    "mlp.train(xs['train'], ys['train'])\n",
    "print('your model accuracy on test data is: %s' % (mlp.test(xs['train'], ys['test'])))\n",
    "mlp.save_model(ASSIGNMENT_PATH / 'topvgg16_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "pBjq-MvamPXO",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "7o4q5LiFiOx1",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instruction:\n",
    "\n",
    "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
    "2. Select File > Save.\n",
    "3. Run **Create Submission** cell, It may take several minutes and it may ask you for your credential.\n",
    "4. Run **Download Submission** cell to obtain your submission as a zip file.\n",
    "5. Grab downloaded file (`dl_asg01__xx__xx.zip`) and submit it via [https://forms.gle/3srwTZhBbc4KfXaR8](https://forms.gle/3srwTZhBbc4KfXaR8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "iWRUf35av3ZP",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Note: ** We need your Github token to create (if doesn't exist previously) new repository to store learned model data. Also Google Drive token enables us to download the current notebook & create a submission. If you are interested feel free to check our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "cTytERc-vlaK",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Create Submission (Run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "r4o37hc3AEUg",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "! pip install -U --quiet PyDrive > /dev/null\n",
    "! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
    "  \n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "from google.colab import files\n",
    "from IPython.display import Javascript\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "asg_name = 'assignment_01'\n",
    "script_save = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "repo_name = 'iust-deep-learning-assignments'\n",
    "submission_file_name = 'dl_asg01__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
    "\n",
    "! tar xf hub-linux-amd64-2.10.0.tgz\n",
    "! cd hub-linux-amd64-2.10.0/ && chmod a+x install && ./install\n",
    "! hub config --global hub.protocol https\n",
    "! hub config --global user.email \"$Your_Github_account_Email\"\n",
    "! hub config --global user.name \"$student_name\"\n",
    "! hub api --flat -X GET /user\n",
    "! hub api -F affiliation=owner -X GET /user/repos > repos.json\n",
    "\n",
    "repos = json.load(open('repos.json'))\n",
    "repo_names = [r['name'] for r in repos]\n",
    "has_repository = repo_name in repo_names\n",
    "if not has_repository:\n",
    "  get_ipython().system_raw('! hub api -X POST -F name=%s /user/repos > repo_info.json' % repo_name)\n",
    "  repo_info = json.load(open('repo_info.json')) \n",
    "  repo_url = repo_info['clone_url']\n",
    "else:\n",
    "  for r in repos:\n",
    "    if r['name'] == repo_name:\n",
    "      repo_url = r['clone_url']\n",
    "  \n",
    "stream = open(\"/root/.config/hub\", \"r\")\n",
    "token = list(yaml.load_all(stream))[0]['github.com'][0]['oauth_token']\n",
    "repo_url_with_token = 'https://'+token+\"@\" +repo_url.split('https://')[1]\n",
    "\n",
    "! git clone \"$repo_url_with_token\"\n",
    "! cp -r \"$ASSIGNMENT_PATH\" \"$repo_name\"/\n",
    "! cd \"$repo_name\" && git add -A\n",
    "! cd \"$repo_name\" && git commit -m \"Add assignment 02 results\"\n",
    "! cd \"$repo_name\" && git push -u origin master\n",
    "\n",
    "sub_info = {\n",
    "    'student_id': student_id,\n",
    "    'student_name': student_name, \n",
    "    'repo_url': repo_url,\n",
    "    'asg_dir_contents': os.listdir(str(ASSIGNMENT_PATH)),\n",
    "    'dateime': str(time.time()),\n",
    "    'asg_name': asg_name\n",
    "}\n",
    "json.dump(sub_info, open('info.json', 'w'))\n",
    "\n",
    "Javascript(script_save)\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
    "\n",
    "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
    "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
    "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
    "\n",
    "print(\"##########################################\")\n",
    "print(\"Done! Submisson created, Please download using the bellow cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "id": "mX9OFzaLtYu_",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Submission (Run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "PUzTlnX1nS8X",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "files.download(submission_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that cell makes an error when running you can download file dl_asg01_your_struden_id_your_name.zip from left panel and files section by right-clicking on it and choosing download button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9KaPEsgPruT"
   },
   "source": [
    "# Special Thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOXZZrFjPxWO"
   },
   "source": [
    "Special thanks to Amirhossein Kazemnejad and Kiamehr Razaee for creating the template of deep learning course assignments."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-woU4Sodh6ND",
    "QfSFVOaKvlDb",
    "toUNxrGEqh3p",
    "zeDLckyY_Yuq",
    "pBjq-MvamPXO",
    "cTytERc-vlaK",
    "mX9OFzaLtYu_",
    "oJLRl0DL5aSO",
    "m9KaPEsgPruT"
   ],
   "name": "deep_assignment_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
