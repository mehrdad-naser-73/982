{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx0DEHvrwYji",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #1 - Multilayer Perceptron\n",
        "\n",
        "\n",
        "Deep Learning / Spring 1399, Iran University of Science and Technology\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWxk7K5TwkRi",
        "colab_type": "text"
      },
      "source": [
        "**Please pay attention to these notes:**\n",
        "<br><br>\n",
        "\n",
        "\n",
        "- **Assignment Due: ** 1398/12/19 23:59:00\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by him/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences. So, please mention his/her name if you have a team-mate.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- Finding any sort of copying will zero down that assignment grade and also will be counted as two negative assignment for your final score.\n",
        "- When you are ready to submit, please follow the instructions at the end of this notebook.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Forum page.\n",
        "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of the depencecies.\n",
        "- **Before starting to work on the assignment Please fill your name in the next section *AND Remember to RUN the cell.* **\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "Assignment Page: [https://iust-deep-learning.github.io/982/assignments/01_Multilayer_Perceptron](https://iust-deep-learning.github.io/982/assignments/01_Multilayer_Perceptron)\n",
        "\n",
        "Course Forum: [https://groups.google.com/forum/#!forum/dl982/](https://groups.google.com/forum/#!forum/dl982/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-j0gHRBy3tV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_deocqkDyxox",
        "colab_type": "text"
      },
      "source": [
        "Fill your information here & run the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHmlUlfVy9ia",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
        "student_id =   0#@param {type:\"integer\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "Your_Github_account_Email = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg01')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJQpO_5YOgf5",
        "colab_type": "text"
      },
      "source": [
        "## 1. MLP from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3v_7iWOr-N",
        "colab_type": "text"
      },
      "source": [
        "In this assignment, you will explore and implement the properties of a primary deep learning model called ***multilayer perceptron(MLP)***. Basically, the goal of an MLP is to learn a non-linear mapping from inputs to outputs. We can show this mapping as $y = f(x; \\theta)$ , where $x$ is the input and $\\theta$ is a vector of all the parameters in the network, which we're trying to learn.  \n",
        "\n",
        "\n",
        "As you see in the figure, every MLP network consists of an input layer, an output layer, and one or more hidden layers in between. Each layer consists of one or more cells called Neurons. In every Neuron, a dot product between the inputs of the cell and a weight vector is calculated. The result of the dot product then goes through a non-linear function (activation function e.g. $tanh$ or $sigmoid$) and gives us the output of the neuron.\n",
        "\n",
        "<center>\n",
        "<img src=https://github.com/mehrdad-naser-73/982/raw/master/static_files/assignments/asg01_assets/pics/MLP.jpg width=\"500\" align=\"center\">\n",
        "</center>\n",
        "\n",
        "\n",
        "<br>\n",
        "Thoughout this assignment, inputs will be matrices with the shape of $b \\times M$ where $b$ is the batch size and $M$ is the number of features of inputs. <br>\n",
        "As for the equations, let's compute the output of the $i$th layer:\n",
        "$$A^i = f(A^{i-1}w^i + b^i)$$\n",
        "\n",
        "Imagine that $(i-1)$th and $i$th layer have sizes of $n$ and $p$ respectively. The dimensions of weight and bias will be as follows:\n",
        "<br><br>\n",
        "$$w^{n\\times p} , b^{1\\times p}$$\n",
        " <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87vFrWhN3bub",
        "colab_type": "text"
      },
      "source": [
        "Numpy is the only package you're allowed to use for implementing your MLP in this assignment, so let's import it in the cell below! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gvunZcHgOe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvoXV-1lLXu-",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpfexSvg3yoK",
        "colab_type": "text"
      },
      "source": [
        "Now let's implement some activation functions! Linear, Relu and Sigmoid are the functions that we'll need in this assignment. Note that you should also implement their derivatives since you'll need them later for back-propagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tfbJgD2GCdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## We've implemented the Linear activation function for you\n",
        "\n",
        "def linear(x, deriv=False):\n",
        "\n",
        "  return x if not deriv else np.ones_like(x)\n",
        "\n",
        "def relu(x, deriv=False):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    x: A numpy array of any shape \n",
        "    deriv: True or False. determines if we want the derivative of the function or not.\n",
        "    \n",
        "  Returns:\n",
        "    relu_out: A numpy array of the same shape as x. \n",
        "      Basically relu function or its derivative applied to every element x\n",
        "               \n",
        "  \"\"\"\n",
        "\n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ######################################## \n",
        "  \n",
        "  \n",
        "  \n",
        "  return relu_out\n",
        "  \n",
        "def  sigmoid(x, deriv=False):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    x: A numpy array of any shape \n",
        "    deriv: True or False. determines if we want the derivative of the function or not.\n",
        "    \n",
        "  Returns:\n",
        "    sig_out: A numpy array of the same shape as x. \n",
        "      Basically sigmoid function or its derivative applied to every element x\n",
        "               \n",
        "  \"\"\"\n",
        "\n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  return sig_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hVlboW1gUtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test your implementation\n",
        "!wget -q https://github.com/mehrdad-naser-73/982/raw/master/static_files/assignments/asg01_assets/act_test.npy\n",
        "\n",
        "x, relu_out, sig_out = np.load('act_test.npy', allow_pickle=True)\n",
        "assert np.allclose( relu_out[0], relu(x, deriv=True), atol=1e-6, rtol=1e-5) and np.allclose(relu_out[1], relu(x, deriv=False), atol=1e-6, rtol=1e-5)\n",
        "assert np.allclose(sig_out[0], sigmoid(x, deriv=True), atol=1e-6, rtol=1e-5) and np.allclose(sig_out[1], sigmoid(x, deriv=False), atol=1e-6, rtol=1e-5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70XLtOxeKEV0",
        "colab_type": "text"
      },
      "source": [
        "**Question**: Why do activation functions have to be non-linear? Could any non-linear function be used as an activation function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WI7RhEkK_fK",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>Write your answers here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2DMSRxfLwkz",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRyGqmxXMkh-",
        "colab_type": "text"
      },
      "source": [
        "Now let's implement our MLP class. This class handles adding layers and doing the forward propagation. Here are the attributes of this class:\n",
        "<br> -  __parameters__: A list of dictionaries in the form of _{'w': weight, 'b': bias}_ where _weight_ and _bias_ are weight matrix and bias vector of a layer.\n",
        "<br>- __act_funcs__: A list of activation functions used in the corresponding layer.\n",
        "<br>- __activations__: A list of matrices each corresponding to the output of each layer.\n",
        "<br>- __layer_ins__: A list of matrices each corresponding to the input of each layer.\n",
        "<br> Note that we store inputs and outputs of the layers because we'll need them later for implementing the back-propagation algorithm. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LSOmEAgONii",
        "colab_type": "text"
      },
      "source": [
        "You only need to complete the _feed_forward_ function in the MLP class. This function performs forward propagation on the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvgYrWCNj11P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP:\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "    \"\"\"\n",
        "  Args:\n",
        "    input_dim: An integer determining the inpu dimension of the MLP\n",
        "               \n",
        "  \"\"\"\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.parameters = []\n",
        "    self.act_funcs = []\n",
        "    self.activations = []\n",
        "    self.layer_ins = []\n",
        "\n",
        "  def add_layer(self, layer_size, act_func=linear):\n",
        "    \"\"\"\n",
        "    Add layers to the MLP using this function\n",
        "  Args:\n",
        "    layer_size: An integer determinig the number of neurons in the layer\n",
        "    act_func: A function applied to the units in the layer \n",
        "    \n",
        "               \n",
        "  \"\"\"\n",
        "    ### Size of the previous layer of mlp\n",
        "    prev_size = self.input_dim if not self.parameters else self.parameters[-1]['w'].shape[-1]\n",
        "\n",
        "    ### Weight scale used in He initialization\n",
        "    weight_scale = np.sqrt(2/prev_size)\n",
        "    ### initializing the weights and bias of the layer\n",
        "    weight = np.random.normal(size=(prev_size, layer_size))*weight_scale\n",
        "    bias = np.ones(layer_size) *0.1\n",
        "    ### Add weights and bias of the layer to the parameters of the MLP\n",
        "    self.parameters.append({'w': weight, 'b': bias})\n",
        "    ### Add the layer's activation function \n",
        "    self.act_funcs.append(act_func)\n",
        "\n",
        "\n",
        "\n",
        "  def feed_forward(self, X):\n",
        "    \"\"\"\n",
        "    Propagate the inputs forward using this function\n",
        "  Args:\n",
        "    X: A numpy array of shape (b, input_dim) where b is the batch size and input_dim is the dimension of the input\n",
        "    \n",
        "  Returns:\n",
        "    mlp_out: A numpy array of shape (b, out_dim) where b is the batch size and out_dim is the dimension of the output\n",
        "\n",
        "    Hint: Don't forget to store inputs and outputs of each layer in self.layer_ins and self.activations respectively\n",
        "               \n",
        "  \"\"\"\n",
        "    self.activations = []\n",
        "    self.layer_ins = []\n",
        "    mlp_out = X\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ######################################## \n",
        "\n",
        "    \n",
        "\n",
        "    return mlp_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcd3RJxPoMWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test your implementation\n",
        "import pickle\n",
        "!wget -q https://github.com/mehrdad-naser-73/982/raw/master/static_files/assignments/asg01_assets/mlptest.pkl\n",
        "\n",
        "x = np.random.normal(size=(512, 100))\n",
        "mlp = MLP(100)\n",
        "mlp.add_layer(64, relu)\n",
        "mlp.add_layer(32, relu)\n",
        "out = mlp.feed_forward(x)\n",
        "assert len(mlp.parameters) == 2\n",
        "assert mlp.activations[0].shape == tuple([512, 64]) and mlp.layer_ins[0].shape == tuple([512, 64])\n",
        "assert mlp.activations[1].shape == tuple([512, 32]) and mlp.layer_ins[1].shape == tuple([512, 32])\n",
        "assert out.shape == tuple([512, 32])\n",
        "assert np.array_equal(mlp.activations[-1], out)\n",
        "\n",
        "x, out, parameters = pickle.load(open('mlptest.pkl', 'rb'))\n",
        "mlp.parameters = parameters\n",
        "assert np.allclose( out, mlp.feed_forward(x), atol=1e-6, rtol=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx2D_UjSs7Ly",
        "colab_type": "text"
      },
      "source": [
        "__Question__: In the _add_layer_ function of the MLP class, we used a method called _He initialization_ to initialize the weights. Explain how this method can help with the training of an MLP?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJX-xv3UxmCn",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>Write your answers here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKSi_6Qkxo4D",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjqaOSDMxuRc",
        "colab_type": "text"
      },
      "source": [
        "In the previous sections, we implemented an MLP that accepts an input $x$ and propagates it forward and produces an output $\\hat{y}$. The next step in implementing our MLP is to see how good our network's output $\\hat{y}$ is compared to the target output $y$! This is where the loss function comes in. This function gets $y$ and $\\hat{y}$ as its inputs and returns a scaler as its output. This scaler indicates how good current parameters of the network are. <br>\n",
        "the choice of this function depends on the task, e.g regression or binary classification. Since you'll be doing a multiclass classification later in this assignment, let's implement the cross-entropy function. Cross-entropy is the function mostly used for classification tasks but to use it in a multiclass setting, the network's outputs must be passed through a softmax activation function and the target output must be in one-hot encoded format.<br>\n",
        "<center>\n",
        "<img src=https://github.com/mehrdad-naser-73/982/raw/master/static_files/assignments/asg01_assets/pics/Capture.PNG width=\"500\" align=\"center\">\n",
        "</center>\n",
        "<br>\n",
        "$$Softmax(\\hat{y})_i =  \\frac{e^{\\hat{y}_i}}{\\sum^{C}_j e^{\\hat{y}_j}} $$ <br>\n",
        "$$ Cross Entropy(y, \\hat{y}) = -\\sum_i^C {y_i log(Softmax(\\hat{y})_i)}$$\n",
        "Where $y$ and $\\hat{y}$ are two one-hot encoded vectors. $y$ is a single target label and $\\hat{y}$ is a single output.<br>\n",
        "Now let's first implement the softmax activation function! Note that the above formulas are for a single sample, however you should implement the batch version!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WGCohQUivOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(y_hat):\n",
        "  \"\"\"\n",
        "    Apply softmax to the inputs\n",
        "  Args:\n",
        "    y_hat: A numpy array of shape (b, out_dim) where b is the batch size and out_dim is the output dimension of the network(number of classes) \n",
        "    \n",
        "  Returns:\n",
        "    soft_out: A numpy array of shape (b, out_dim)\n",
        "               \n",
        "  \"\"\"\n",
        "  \n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################\n",
        "\n",
        "  return soft_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtSCdCQT9GAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test your implementation\n",
        "\n",
        "y_hat = np.random.normal(size=(100, 5))\n",
        "y_soft = softmax(y_hat)\n",
        "assert y_hat.shape == y_soft.shape\n",
        "assert all([(y - 1.)<1e-5 for y in np.sum(y_soft, axis=1)])\n",
        "y_hat = np.array([[10,10,10,10], [0,0,0,0]])\n",
        "assert np.allclose( softmax(y_hat), np.array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]]), atol=1e-6, rtol=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRsstrQQHXYu",
        "colab_type": "text"
      },
      "source": [
        "Now implement the categorical cross-entropy function (\"categorical\" refers to multiclass classification). Note that the inputs are in batches, so the loss of a batch of samples will be the average of losses of samples in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt9YcJAr8ADF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_cross_entropy(y, y_soft):\n",
        "  \"\"\"\n",
        "    Compute the categorical cross entropy loss\n",
        "  Args:\n",
        "    y: A numpy array of shape (b, out_dim). Target labels of network.\n",
        "    y_soft: A numpy array of shape (b, out_dim). Output of the softmax activation function\n",
        "    \n",
        "  Returns:\n",
        "    loss: A scaler of type float. Average loss over a batch.\n",
        "\n",
        "  Hint: Use np.mean to compute average loss of a batch\n",
        "               \n",
        "  \"\"\"\n",
        "\n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL_BF1V49Gwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test your implementation\n",
        "\n",
        "y = np.array([[1,0,0], [0,0,1], [1,0,0], [0,1,0]])\n",
        "y_hat = np.array([[10,1,1], [0,-1,9], [100,-9,9], [0.1,12,10]])\n",
        "y_soft = softmax(y_hat)\n",
        "assert round(categorical_cross_entropy(y, y_soft), 3) == 0.032\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQyCUJ0jSRUf",
        "colab_type": "text"
      },
      "source": [
        "Great! You have implemented both softmax and categorical cross-entropy functions. Now instead of applying softmax activation function to the output layer of the MLP and then using categorical cross-entropy as loss function, we can merge these two steps and make a softmax categorical cross-entropy loss function and use linear activation function in the output layer! The reason behind this is that the gradient of the softmax categorical cross-entropy loss with respect to the MLP's output is efficiently calculated as:\n",
        "<br>\n",
        "\n",
        "$$ Softmax(\\hat{y}) - y$$\n",
        "\n",
        "for a single sample. Here $\\hat{y}$ is the MLP's output and $y$ is the target output (labels).<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkZZovW27k3T",
        "colab_type": "text"
      },
      "source": [
        "Now let's implement the softmax categorical cross-entropy function!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzbOtjnJMohT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_categorical_cross_entropy(y, y_hat, return_grad=False):\n",
        "  \"\"\"\n",
        "    Compute the softmax categorical cross entropy loss\n",
        "  Args:\n",
        "    y: A numpy array of shape (b, out_dim). Target labels of network.\n",
        "    y_hat: A numpy array of shape (b, out_dim). Output of the output layer of the network\n",
        "    return_grad: If True return gradient of the loss with respect to y_hat. If False just return the loss\n",
        "    \n",
        "  Returns:\n",
        "    loss: A scaler of type float. Average loss over a batch.\n",
        "               \n",
        "  \"\"\"\n",
        "  \n",
        "  y_soft = softmax(y_hat)\n",
        "  \n",
        "  if not return_grad:\n",
        "    loss = categorical_cross_entropy(y, y_soft)\n",
        "    return loss\n",
        "  else:\n",
        "    loss_grad = (y_soft - y)/y.shape[0]\n",
        "    return loss_grad\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CenxHCNZD-5V",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Back-Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66l6gCb9EDvW",
        "colab_type": "text"
      },
      "source": [
        "After calculating the loss of the MLP, we need to propagate this loss back to the hidden layers in order to calculate the gradient of the loss with respect to the weights and biases of the network. The algorithm used to calculate these gradients is called back-propagation or simply backprop. Backprop uses chain rule to compute the gradients of the network parameters. Now let's go over the steps of this algorithm (This is the fully matrix-based version):\n",
        "- calculate gradient of the loss with respect to $\\hat{y}$\n",
        "<br> $g \\longleftarrow \\nabla_\\hat{y} Loss$ \n",
        "- for each layer $L$ starting from the ouput layer: <br>\n",
        "&emsp;&emsp; $g \\longleftarrow g \\odot f^\\prime(input^{(L)})$ &emsp; ($input^{(L)}$ is the input of $L$th layer and $f$ is the activation function)<br>\n",
        "&emsp;&emsp; $\\nabla_{b^{(L)}}Loss \\longleftarrow \\sum_i^{batch} {g_i}$ <br>\n",
        "&emsp;&emsp; $\\nabla_{w^{(L)}}Loss \\longleftarrow output^{(L-1)T}g$ &emsp; ($output^{(L-1)}$ is the output of $(L-1)$th layer ) <br>\n",
        "&emsp;&emsp; $g \\longleftarrow gw^{(L)T}$\n",
        "\n",
        "Check [this](http://neuralnetworksanddeeplearning.com/chap2.html) for a detailed explanation of the back-propagation algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-bV6UF0mJMC",
        "colab_type": "text"
      },
      "source": [
        "Now implement the back-propagation algorithm!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-pJJYYHgyKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp_gradients(mlp, loss_function, x, y):\n",
        "  \"\"\"\n",
        "    Compute the gradient of loss with respect to mlp's weights and biases\n",
        "  Args:\n",
        "    mlp: An object of MLP class\n",
        "    loss_function: A function used as loss function of the mlp\n",
        "    x: A numpy array of shape (batch_size, input_dim). The mlp's input\n",
        "    y: A numpy array of shape (batch_size, num_classes). Target labels\n",
        "    \n",
        "  Returns:\n",
        "    gradients: A list of dictionaries {'w': dw, 'b': db} corresponding to the dictionaries in mlp.parameters\n",
        "        dw is the gradient of loss with respect to the weights of the layer \n",
        "        db is the gradient of loss with respect to the bias of the layer \n",
        "               \n",
        "  \"\"\"  \n",
        "\n",
        "  gradients = []\n",
        "\n",
        "  ### get the output of the network\n",
        "  y_hat = mlp.activations[-1]\n",
        "  num_layers = len(mlp.parameters)\n",
        "\n",
        "  ### compute gradient of the loss with respect to network output\n",
        "  g = loss_function(y, y_hat, return_grad=True)\n",
        "\n",
        "  ### You'll need the input in the last step of backprop so let's make a new list with x in the beggining\n",
        "  activations = [x] + mlp.activations \n",
        "  \n",
        "  for i in reversed(range(num_layers)):\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "    \n",
        "  return gradients\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM9cRmC8mvMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test your implementation\n",
        "\n",
        "import pickle\n",
        "!wget -q https://github.com/mehrdad-naser-73/982/raw/master/static_files/assignments/asg01_assets/grad_test.zip\n",
        "!unzip grad_test.zip\n",
        "x = np.load('grad_x.npy')\n",
        "y = np.load('grad_y.npy')\n",
        "mlp = pickle.load(open('grad_mlp_test.pkl', 'rb'))\n",
        "expected_grads = pickle.load(open('grads', 'rb'))\n",
        "mlp.feed_forward(x)\n",
        "grads = mlp_gradients(mlp, softmax_categorical_cross_entropy, x, y)\n",
        "assert all([np.allclose(eg['w'], g['w'], atol=1e-6, rtol=1e-5) and \n",
        "            np.allclose(eg['b'], g['b'], atol=1e-6, rtol=1e-5) \n",
        "            for eg, g in zip(expected_grads, grads)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPjKQSS6mi_Q",
        "colab_type": "text"
      },
      "source": [
        "### 1.5 Optimizaion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe0IJelEmmXw",
        "colab_type": "text"
      },
      "source": [
        "Now that we've computed the gradients of the parameters of our MLP, we should optimize these parameters using the gradients in order for the network to produce better outputs. <br> \n",
        "Gradient descent is an optimizaion method that iteratively moves the paramters in the oposite direction of their gradients. Below is the update rule for gradient descent:\n",
        "<br><br>\n",
        "$$ w \\leftarrow w - \\alpha \\nabla_wLoss$$ \n",
        "<br>\n",
        "Where $\\alpha$ is the learning rate hyperparameter.<br>\n",
        "There are three main variants of gradient descent: stochastic gradient descent, mini-batch gradient descent and batch gradient descent. <br>\n",
        "Mini-batch gradient descent is the most used variant in practice and that's what we'll use in this assignment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOx9iK3G4bef",
        "colab_type": "text"
      },
      "source": [
        "Let's perform a step of gradient descent on a simple MLP!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxleIKcO4ama",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.random.normal(size=(16, 10))\n",
        "y = np.eye(16)\n",
        "lr = 0.1\n",
        "### Define the mlp \n",
        "mlp = MLP(x.shape[-1])\n",
        "mlp.add_layer(16)\n",
        "mlp.add_layer(8)\n",
        "mlp.add_layer(y.shape[-1])\n",
        "### compute mlp's output\n",
        "y_hat = mlp.feed_forward(x)\n",
        "### print current loss\n",
        "print(\"loss before gradient descent: \", softmax_categorical_cross_entropy(y, y_hat))\n",
        "### Compute gradients of the mlp's parameters \n",
        "grads = mlp_gradients(mlp, softmax_categorical_cross_entropy, x, y)\n",
        "### perform gradient descent\n",
        "mlp.parameters = [{'w':p['w']-lr*g['w'], 'b':p['b']-lr*g['b']} for g, p in zip(grads, mlp.parameters)]\n",
        "### compute mlp's output again after gradeint descent\n",
        "y_hat = mlp.feed_forward(x)\n",
        "### print loss after gradient descent\n",
        "print(\"loss after gradient descent: \", softmax_categorical_cross_entropy(y, y_hat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFsBdy9v7iFs",
        "colab_type": "text"
      },
      "source": [
        "__Question__: Do gradient descent steps always decrease the loss? why?   (Hint: toy with the learning rate in the axample above!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYYgkfw58fm5",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>Write your answers here</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7mtWfd88kbA",
        "colab_type": "text"
      },
      "source": [
        "Instead of using gradient descent, we'll be using an extention of it called gradient descent with momentum. So instead of updating the parameters based only on current gradients, we take into account the gradients from previous steps! This way, parameter updates will have lower variance and convergence will be faster and smoother. \n",
        "$$ v \\leftarrow \\gamma  v - \\alpha \\nabla_wLoss$$ \n",
        "$$ w \\leftarrow w + v$$\n",
        "Where $w$ is denotes mlp's weights and $v$ is called velocity which is basically a weighted average of all previous gradients.<br>\n",
        "Here $\\gamma$ determines how fast effects of the previous gradients fade and $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s9BFOPoMDoL",
        "colab_type": "text"
      },
      "source": [
        "Now let's implement the SGD class!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8EW1X-FNmfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SGD:\n",
        "\n",
        "  def __init__(self, lr=0.01, momentum=0.9):\n",
        "    \"\"\"\n",
        "  Args:\n",
        "    lr: learning rate of the SGD optimizer\n",
        "    momentum: momentum of the SGD optimizer\n",
        "\n",
        "    Hint: velocity should be a list of dictionaries just like mlp.parameters\n",
        "               \n",
        "  \"\"\" \n",
        "\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    ### initialize velocity\n",
        "    self.velocity = []\n",
        "  \n",
        "  def step(self, parameters, grads):\n",
        "\n",
        "    \"\"\"\n",
        "    Perform a gradient descent step\n",
        "  Args:\n",
        "    parameters: A list of dictionaries {'w': weights , 'b': bias}. MLP's parameters. \n",
        "    grads: A list of dictionaries {'w': dw, 'b': db}. gradient of MLP's parameters. Basically the output of \"mlp_gradients\" function you implemented!\n",
        "    \n",
        "  Returns:\n",
        "    Updated_parameters: A list of dictionaries {'w': weights , 'b': bias}. mlp's parameters after performing a step of gradient descent. \n",
        "               \n",
        "  \"\"\"\n",
        "\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ######################################## \n",
        "\n",
        "    \n",
        "\n",
        "    return Updated_parameters\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDqMMm-pNlVF",
        "colab_type": "text"
      },
      "source": [
        "## 2. Classifying Kannada Handwritten Digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpTgQUZ7Ofcw",
        "colab_type": "text"
      },
      "source": [
        "In this part of the assignment, you'll use the MLP you implemented in the first part to classify Kannada handwritten digits!<br> This dataset consists of 60000 images of handwritten digits in Kannada script.<br>\n",
        "You can check [this](https://github.com/vinayprabhu/Kannada_MNIST) github repository for more information about the dataset.\n",
        "let's download the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7J5AavbPYUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://github.com/mehrdad-naser-73/982/raw/master/static_files/assignments/asg01_assets/Kannada.zip\n",
        "!unzip Kannada.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9vIApVkPgMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "train = pd.read_csv('train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXQcevYVPpxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtLGkfCJQTTb",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the first column of the dataframe is the label, and the rest of the columns are the pixels. Let's put the dataset in numpy arrays. Also, we must normalize the pixel values to [0,1] range to help the convergence of our MLP model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDDtw13gQs--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = train.values[:, 1:]/255.\n",
        "y = train.values[:, 0]\n",
        "plt.imshow(x[10000].reshape(28, 28))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJgwvpb5Xec4",
        "colab_type": "text"
      },
      "source": [
        "As we are doing a multiclass classification, the labels must be in one-hot encoded format. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB8TavCl7TSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoder(y):\n",
        "\n",
        "  y = y.reshape(-1)\n",
        "  num_samples = y.shape[0]\n",
        "  max_label = np.max(y)\n",
        "  one_hot = np.zeros((num_samples, max_label+1))\n",
        "  one_hot[np.arange(num_samples),y] = 1\n",
        "  \n",
        "  return one_hot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3c9FP4ZdZvv",
        "colab_type": "text"
      },
      "source": [
        "Now let's transform the labels into one-hot encoded format!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DuFO6g3dWgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = one_hot_encoder(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDWfbIPzaUTB",
        "colab_type": "text"
      },
      "source": [
        "We've implemented the _get_mini_batches_ function below. This function transforms the dataset into multiple batches. We need this function because we'll be doing mini-batch gradient descent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQwrWQdR8wCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def get_mini_batches(x, y, batch_size, shuffle=True):\n",
        "\n",
        "  idx = list(range(len(x)))\n",
        "  np.random.shuffle(idx)\n",
        "  steps = math.ceil(len(x)/batch_size)\n",
        "  x, y = x[idx, :], y[idx, :]\n",
        "  for i in range(steps):\n",
        "    yield (x[i*batch_size: (i+1)*batch_size], y[i*batch_size: (i+1)*batch_size])\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iQ_J2VXf2M_",
        "colab_type": "text"
      },
      "source": [
        "Evaluation metrics are used to measure the performance of a model after training. The choice of this metric depends on factors like the nature of the task (e.g classification or regression) or a dataset's characteristics (e.g class imbalance). For multiclass classification with balanced classes, accuracy is a reasonable choice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiu967nZiU76",
        "colab_type": "text"
      },
      "source": [
        "We've implemented the accuracy function in the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JWfurLbvjDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y, y_hat):\n",
        "\n",
        "  return np.mean(np.argmax(y, axis=-1)==np.argmax(y_hat, axis=-1))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RcB5fPVibUG",
        "colab_type": "text"
      },
      "source": [
        "Now let's split the dataset into train and validatoin sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWUJ-g8bT_JC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7utzA1JimT0",
        "colab_type": "text"
      },
      "source": [
        "Everything is now ready for training our MLP! Create your MLP model in the cell bellow. The choice of the number of layers, their sizes and their activation functions is up to you.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHhoNHa3vz4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "mlp = MLP(x_train.shape[-1])\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0-8HUbYkXCQ",
        "colab_type": "text"
      },
      "source": [
        "Let's set some hyper-parameters. Feel free to change these hyper-parameters however you see fit!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3YZeR-pkBI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "Batch_size = 1024\n",
        "sgd_lr = 0.1\n",
        "sgd_momentum = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6RF5pttlG8l",
        "colab_type": "text"
      },
      "source": [
        "Now let's train the network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOUEaeTxjTIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook\n",
        "### Defining a optimizer\n",
        "optimizer = SGD(lr=sgd_lr, momentum=sgd_momentum)\n",
        "\n",
        "train_loss, val_loss, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "for i in range(epochs):\n",
        "  mini_batches = get_mini_batches(x_train, y_train, Batch_size)\n",
        "  for xx, yy in tqdm_notebook(mini_batches, desc='epoch {}'.format(i+1)):\n",
        "\n",
        "    ### forward propagation\n",
        "    mlp.feed_forward(xx)\n",
        "    ### compute gradients\n",
        "    grads = mlp_gradients(mlp, softmax_categorical_cross_entropy, xx, yy)\n",
        "    ### optimization\n",
        "    mlp.parameters = optimizer.step(mlp.parameters, grads)\n",
        "    \n",
        "  y_hat = mlp.feed_forward(x_train)\n",
        "  y_hat_val = mlp.feed_forward(x_val)\n",
        "  val_loss.append(softmax_categorical_cross_entropy(y_val, y_hat_val))\n",
        "  train_loss.append(softmax_categorical_cross_entropy(y_train, y_hat))\n",
        "  train_acc = accuracy(y_train, y_hat)*100\n",
        "  val_acc = accuracy(y_val, y_hat_val)*100\n",
        "  train_accs.append(train_acc)\n",
        "  val_accs.append(val_acc)\n",
        "  print(\"training acc: {:.2f} %\".format(train_acc))\n",
        "  print(\"test acc: {:.2f} %\".format(val_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzrzvDPXnLlI",
        "colab_type": "text"
      },
      "source": [
        "Let's visualize accuracy and loss for train and validation sets during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_rB9k9d2f81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(list(range(len(train_loss))), train_loss, label='train')\n",
        "plt.plot(list(range(len(val_loss))), val_loss, label='val')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP3Rgp99o-f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(list(range(len(train_accs))), train_accs, label='train')\n",
        "plt.plot(list(range(len(val_accs))), val_accs, label='val')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu0QS_stnsAX",
        "colab_type": "text"
      },
      "source": [
        "__Question__: Looking at loss and accuracy plots, how would you describe your model in terms of bias and variance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFA-4VdmoL4E",
        "colab_type": "text"
      },
      "source": [
        "<font color=red>Write your answers here</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgC79dlicyhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb6QUf5SxigI",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Sb-xU2xpNM",
        "colab_type": "text"
      },
      "source": [
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instructions:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. Run **Make Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "4. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "5. Grab the downloaded file (`dl_asg01__xx__xx.zip`) and upload it via https://forms.gle/2dogVcZhfBvBC1aM6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOklAtKSyYLj",
        "colab_type": "text"
      },
      "source": [
        "**Note: ** We need your Github token to create (if doesn't exist previously) new repository to store learned model data. Also Google Drvie token enable us to download current notebook & create submission. If you are intrested feel free to check our code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QBH-aiHyeus",
        "colab_type": "text"
      },
      "source": [
        "## Make Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h64ZtRpynhH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "  \n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'assignment_1'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'dl_asg01__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "\n",
        "! tar xf hub-linux-amd64-2.10.0.tgz\n",
        "! cd hub-linux-amd64-2.10.0/ && chmod a+x install && ./install\n",
        "! hub config --global hub.protocol https\n",
        "! hub config --global user.email \"$Your_Github_account_Email\"\n",
        "! hub config --global user.name \"$student_name\"\n",
        "! hub api --flat -X GET /user\n",
        "! hub api -F affiliation=owner -X GET /user/repos > repos.json\n",
        "\n",
        "repos = json.load(open('repos.json'))\n",
        "repo_names = [r['name'] for r in repos]\n",
        "has_repository = repo_name in repo_names\n",
        "if not has_repository:\n",
        "  get_ipython().system_raw('! hub api -X POST -F name=%s /user/repos > repo_info.json' % repo_name)\n",
        "  repo_info = json.load(open('repo_info.json')) \n",
        "  repo_url = repo_info['clone_url']\n",
        "else:\n",
        "  for r in repos:\n",
        "    if r['name'] == repo_name:\n",
        "      repo_url = r['clone_url']\n",
        "  \n",
        "stream = open(\"/root/.config/hub\", \"r\")\n",
        "token = list(yaml.load_all(stream))[0]['github.com'][0]['oauth_token']\n",
        "repo_url_with_token = 'https://'+token+\"@\" +repo_url.split('https://')[1]\n",
        "\n",
        "! git clone \"$repo_url_with_token\"\n",
        "! cp -r \"$ASSIGNMENT_PATH\" \"$repo_name\"/\n",
        "! cd \"$repo_name\" && git add -A\n",
        "! cd \"$repo_name\" && git commit -m \"Add assignment 01 results\"\n",
        "! cd \"$repo_name\" && git push -u origin master\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'repo_url': repo_url,\n",
        "    'asg_dir_contents': os.listdir(str(ASSIGNMENT_PATH)),\n",
        "    'dateime': str(time.time()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRyW0Jx4yiEi",
        "colab_type": "text"
      },
      "source": [
        "## Download Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svzPnugYztkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(submission_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzGIXOxfrhZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}